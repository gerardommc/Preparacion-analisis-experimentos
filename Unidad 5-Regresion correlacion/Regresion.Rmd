---
title: "Regresión lineal"
author: "Gerardo Martín"
date: "5/4/2021"
output:
      bookdown::html_document2:
            toc: true 
            number_sections: true
            toc_float: true
            theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

Todos los ejmplos de análisis que hemos visto hasta la fecha han sido sobre comparaciones de medias, es decir de cómo diferentes tratamientos estrictamente categóricos, afectan el valor más probable de ser observado. Categóricos pueden considerarse variables como:

1. Diferentes fertilizantes
2. Diferentes tipos de suelo
3. Diferentes medicamentos
4. Diferentes especies de organismos

Ninguna de estas variables, como tal, puede considerarse con un valor numérico, por lo que sus efectos sólo pueden ser estimados para los grupos experimentales. Las variables contínuas, a diferencia de las categóricas, sí pueden tener valores numéricos, de modo que es posible estimar valores numéricos para sus efecos. Ejemplos de variables contínuas son:

1. Temperatura
2. Dosis de medicamento
3. Concentración de contaminantes en el agua

Como es de esperarse, las variables contínuas también pueden representarse como categóricas. Por ejemplo se puede comparar la velocidad de crecimiento de una especie de crutáceo a dos o tres temperaturas. Cuando el número de valores de temperatura es lo suficientemente grande, sin embargo, resulta más interesante estimar por ejemplo, cómo cambia la velocidad de crecimiento con el el cambio de temperatura, como en una ecuación lineal:

\begin{equation}
\mathrm{Crecimiento} = \mathrm{Intercepto} + \mathrm{Tasa} \times 
\mathrm{Temperatura} (\#eq:crecimiento)
\end{equation}


Donde no hay ningún tipo de restricción sobre el valor que $\mathrm{Tasa}$ pueda tomar, es decir puede ir desde $-\infty$ hasta $+ \infty$. La regresión lineal, tiene como objetivo, justamente ese, estimar el valor de $\mathrm{Tasa}$ para cualquier par de variables numéricas $x$ y $y$. De manera más general, la ecuación \@ref(eq:crecimiento) puede ser representada como:

\begin{equation}
y(x) = \beta_0 + \beta_1 x (\#eq:lineal)
\end{equation}

Al igual que en ANOVA, puede existir cualquier número de variables $x$ de tal modo que $y$ puede ser una función $y(x_1, x_2, \dots, x_n)$, siempre y cuando existan grados de libertad suficientes de $y$, y existan observaciones de $x$ para cada todas y cada una de las $y$. 

# Ejemplos de diseños experimentales con factores contínuos

La base de datos `cars` de cualquier instalación de **R** es ya, un ejemplo clásico de una relación lineal entre dos variables aleatorias, la cual muestra con 50 observaciones la distancia recorrida por un auto hasta alcanzar una velocidad de 0 km/h mientras viaja a una velocidad establecida (figura \@ref(fig:cars-1)). 

```{r cars-1, echo=F, fig.align='center', fig.cap="Relacion entre velocidad y distancia recorrida de frenado en `cars`.", fig.height=4, fig.width=4, message=FALSE}
library(ggplot2)
ggplot(cars) + geom_point(aes(x =  speed, y = dist)) + 
      labs(x = "Velocidad", y = "Distancia")
```

Una regresión lineal que represente a la distancia de frenado como función de la velocidad, estimaría dos parámetros, $\beta_0$ y $\beta_1$ (ecuación \@ref(eq:lineal)). $\beta_0$ corresponde al intercepto, que es el valor de $y$ cuando $x=0$, y el segundo es $\beta_1$, el cambio de unidades de $y$ por unidad de cambio de $x$, o la pendiente de la recta:

```{r cars-2, echo=F, fig.align='center', fig.cap="Recta de regresión entre velocidad y distancia de frenado en `cars`.", fig.height=4, fig.width=4, message=FALSE}
library(ggplot2)
ggplot(cars) + geom_point(aes(x =  speed, y = dist)) + 
      geom_smooth(aes(x = speed, y = dist), method = "lm") +
      labs(x = "Velocidad", y = "Distancia")
```

Para estimar estos parámetros en **R**, utilizamos la función `lm`, poniendo atención a la variable dependiente (lado izquierdo, `dist`) e independiente (lado derecho, `speed`):

```{r}
data(cars)

m1 <- lm(dist ~ speed, cars)
summary(m1)
```

El significado de los coeficientes en la columna `Estimate` es el descrito arriba, sin embargo, estos adquieren implicaciones adicionales en estadística. Por ejemplo, podemos que que el intercepto tiene un valor de -17.59, lo cual es, en términos del fenómeno físico, falso, pues en este caso sólo es relevante para los datos analizados, en decir para velocidades de 5-25 km/h, con el (los) automóvil(es) utilizado(s) en esas mediciones. El parámetro $\beta_1$ para speed, es un poco más generalizable, y quiere decir que **en promedio**, por cada km/h adicioanl de velocidad, necesitamos 3.93m más para frenar por completo el automóvil. Las estimaciones de significancia estadística indican qué tan probable es que $\beta_{0, 1} = 0$, es decir, qué tan probable es que el efecto de $x$ sobre $y$ sea 0

## Varianza explicada en regresión lineal

Una medida muy importante en regresión lineal es $R^2$, que indica qué tan cerca pasa la línea. dibujada por la ecuación, de los datos observados. Veamos por ejemplo, que en la figura \@ref(fig:cars-2), la línea de regresión pasa por en medio de la nube de puntos, y la mayoría de estos se encuentran a una distancia determinada de la línea. $R^2$ es justamente una medida de la distancia promedio de los puntos a la línea tal que:

1. $R^2 = 0$ indica que la variable independente no explica a la variable dependiente
2. $R^2 = 1$ indica que la variable dependiente explica perfectamente a la variable dependiente

En **R**, el valor de $R^2$ lo encontramos en la penúltima línea del `summary`, en análisis de `cars`, tenemos:

`Multiple R-squared:  0.6511,	Adjusted R-squared:  0.6438`

dos valores de $R^2$, y como es de esperarse, es mejor confiar en el método más conservador de la $R^2$ ajustada. En el caso de `cars`, la velocidad explica una buena parte de la distancia de frenado.

# ¿Cómo se hace una regresión lineal?

El proceso de una regresión lineal involucra tres estimaciones secuenciales:

1. Coeficientes de regresión $\beta_0$ y $\beta_1$
2. $R^2$
3. Significancia estadística

A continuación se describe el proceso para cada uno de los pasos

## Estimación de los coeficientes de regresión

De acuerdo con las convenciones matemáticas en este tutorial $x$ es la variable independiente $y$ es la dependiente, por lo que $y(x) = f(x)$.

El primer paso para calcular los coeficientes es obtener las sumas (operador $\sum$):

- $\sum_{i = 1}^n x_i = x_1 + x_2 + \dots + x_n$
- $\sum_{i = 1}^n y_i = y_1 + y_2 + \dots + y_n$
- $\sum_{i = 1}^n x_i y_i = x_1 \times y_1 + x_2 \times y_2 + \dots + x_n \times y_n$
- $\sum_{i = 1}^n x_i^2 = x_1^2 + x_2^2 + \dots + x_n^2$
- $\sum_{i = 1}^n y_i^2 = y_1^2 + y_2^2 + \dots + y_n^2$

Una vez obtenidas estas sumas, las utilizaremos para estimar $\beta_0$ y $\beta_1$:

\begin{equation}
      \beta_0 = \frac{ \sum y \cdot \sum x^2 – \sum x \cdot \sum xy} {n \cdot \sum x^2 – \left(\sum x \right)^2} (\#eq:beta0)
\end{equation}

\begin{equation}
      \beta_1 = \frac{n\cdot \sum xy - \sum x \cdot \sum y}{n \cdot \sum x^2 - \left( \sum x \right)^2} (\#eq:beta1)
\end{equation}

## Estimación de $R^2$

El valor de $R^2$ expresa la proporción de varianza explicada por la regresión lineal, de modo que se obtiene de restar el conciente de la suma de cuadrados del error y la suma de cuadrados totales de la unidad (varianza total):

\begin{equation} R^2 = 1 - \frac{SS_{Error}}{SS_{Totales}} \end{equation}

$SS_{Error}$ es la varianza de los residuales, es decir, la variabilidad que la regresión no logró explicar, y $SS_{Totales}$ es la varianza cruda:

\begin{equation}
   SS_{Error} = \sum y^2 - \beta_0 \sum y - \beta_1 \sum x y (\#eq:SS-err)
\end{equation}

\begin{equation}
   SS_{Total} = \sum y^2 - \frac{\left( \sum y \right)^2}{n} (\#eq:SS-tot)
\end{equation}


## Estimación de significancia estadística

[Regresar al índice del curso](../index.html)